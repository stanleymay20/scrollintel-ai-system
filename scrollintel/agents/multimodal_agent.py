"""
MultimodalAgent - Cross-Modal Intelligence Agent
Combines vision, audio, text, and code inputs for unified AI processing.
"""

import asyncio
import base64
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Union, Tuple
from uuid import uuid4
from dataclasses import dataclass
from enum import Enum
import logging

from scrollintel.core.interfaces import BaseAgent, AgentType, AgentRequest, AgentResponse, AgentCapability, ResponseStatus
from scrollintel.engines.multimodal_engine import MultimodalEngine, ModalityType, FusionStrategy, MultimodalTask

logger = logging.getLogger(__name__)


class MultimodalCapability(str, Enum):
    """Specific multimodal capabilities."""
    IMAGE_UNDERSTANDING = "image_understanding"
    AUDIO_PROCESSING = "audio_processing"
    VIDEO_ANALYSIS = "video_analysis"
    CODE_ANALYSIS = "code_analysis"
    CROSS_MODAL_SEARCH = "cross_modal_search"
    MULTIMODAL_GENERATION = "multimodal_generation"
    CONTENT_FUSION = "content_fusion"
    SEMANTIC_ALIGNMENT = "semantic_alignment"


class ProcessingMode(str, Enum):
    """Processing modes for multimodal content."""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    HIERARCHICAL = "hierarchical"
    ADAPTIVE = "adaptive"


@dataclass
class MultimodalInput:
    """Structured multimodal input."""
    id: str
    modalities: Dict[ModalityType, Any]
    metadata: Dict[str, Any]
    processing_hints: Dict[str, Any]
    priority_modality: Optional[ModalityType] = None
    fusion_strategy: FusionStrategy = FusionStrategy.LATE_FUSION
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()


@dataclass
class MultimodalOutput:
    """Structured multimodal output."""
    id: str
    input_id: str
    results: Dict[str, Any]
    modality_contributions: Dict[ModalityType, float]
    fusion_quality: float
    processing_time: float
    confidence: float
    generated_content: Dict[ModalityType, Any] = None
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()
        if self.generated_content is None:
            self.generated_content = {}


class MultimodalAgent(BaseAgent):
    """Advanced multimodal agent for cross-modal intelligence."""
    
    def __init__(self):
        super().__init__(
            agent_id="multimodal-agent",
            name="Multimodal Agent",
            agent_type=AgentType.AI_ENGINEER
        )
        
        self.capabilities = [
            AgentCapability(
                name="multimodal_understanding",
                description="Understand and analyze content across multiple modalities",
                input_types=["text", "image", "audio", "video", "code"],
                output_types=["unified_analysis", "cross_modal_insights"]
            ),
            AgentCapability(
                name="cross_modal_retrieval",
                description="Search and retrieve content across different modalities",
                input_types=["query", "modality_preferences"],
                output_types=["ranked_results", "similarity_scores"]
            ),
            AgentCapability(
                name="content_generation",
                description="Generate content in one modality based on input from others",
                input_types=["source_content", "target_modality"],
                output_types=["generated_content", "quality_metrics"]
            ),
            AgentCapability(
                name="semantic_alignment",
                description="Align semantic representations across modalities",
                input_types=["multimodal_content"],
                output_types=["aligned_representations", "alignment_quality"]
            )
        ]
        
        # Initialize multimodal engine
        self.multimodal_engine = MultimodalEngine()
        
        # Processing state
        self.active_sessions = {}
        self.processing_history = []
        self.modality_preferences = {}
        
        # Content generators for different modalities
        self.content_generators = {
            ModalityType.TEXT: self._generate_text_content,
            ModalityType.IMAGE: self._generate_image_description,
            ModalityType.AUDIO: self._generate_audio_description,
            ModalityType.CODE: self._generate_code_explanation
        }
    
    async def initialize(self):
        """Initialize the multimodal agent."""
        await self.multimodal_engine.initialize()
    
    async def process_request(self, request: AgentRequest) -> AgentResponse:
        """Process multimodal requests."""
        start_time = asyncio.get_event_loop().time()
        
        try:
            prompt = request.prompt.lower()
            context = request.context or {}
            
            if "understand" in prompt or "analyze" in prompt:
                content = await self._understand_multimodal_content(request.prompt, context)
            elif "search" in prompt or "retrieve" in prompt:
                content = await self._cross_modal_search(request.prompt, context)
            elif "generate" in prompt or "create" in prompt:
                content = await self._generate_multimodal_content(request.prompt, context)
            elif "align" in prompt or "match" in prompt:
                content = await self._align_semantic_representations(request.prompt, context)
            elif "compare" in prompt or "similarity" in prompt:
                content = await self._compare_multimodal_content(request.prompt, context)
            else:
                content = await self._general_multimodal_processing(request.prompt, context)
            
            execution_time = asyncio.get_event_loop().time() - start_time
            
            return AgentResponse(
                id=f"multimodal-{uuid4()}",
                request_id=request.id,
                content=content,
                artifacts=[],
                execution_time=execution_time,
                status=ResponseStatus.SUCCESS
            )
            
        except Exception as e:
            execution_time = asyncio.get_event_loop().time() - start_time
            return AgentResponse(
                id=f"multimodal-{uuid4()}",
                request_id=request.id,
                content=f"Error in multimodal processing: {str(e)}",
                artifacts=[],
                execution_time=execution_time,
                status=ResponseStatus.ERROR,
                error_message=str(e)
            )
    
    async def _understand_multimodal_content(self, prompt: str, context: Dict[str, Any]) -> str:
        """Understand and analyze multimodal content."""
        multimodal_input = await self._parse_multimodal_input(context)
        
        # Process through multimodal engine
        engine_result = await self.multimodal_engine.process(
            multimodal_input.modalities,
            {
                "task": MultimodalTask.MULTIMODAL_CLASSIFICATION,
                "fusion_strategy": multimodal_input.fusion_strategy
            }
        )
        
        # Generate comprehensive analysis
        analysis = await self._generate_comprehensive_analysis(
            multimodal_input, engine_result
        )
        
        # Store processing result
        output = MultimodalOutput(
            id=f"output-{uuid4()}",
            input_id=multimodal_input.id,
            results=engine_result,
            modality_contributions=await self._calculate_modality_contributions(engine_result),
            fusion_quality=engine_result.get("fusion_quality", 0.8),
            processing_time=0.5,  # Would be calculated
            confidence=engine_result.get("confidence", 0.75)
        )
        
        self.processing_history.append(output)
        
        return f"""
# Multimodal Content Analysis

## Input Summary
- **Modalities Detected**: {list(multimodal_input.modalities.keys())}
- **Fusion Strategy**: {multimodal_input.fusion_strategy.value}
- **Processing Mode**: {context.get('processing_mode', 'adaptive')}

## Analysis Results
{analysis}

## Modality Contributions
{await self._format_modality_contributions(output.modality_contributions)}

## Cross-Modal Insights
{await self._generate_cross_modal_insights(multimodal_input, engine_result)}

## Confidence Assessment
- **Overall Confidence**: {output.confidence:.2f}
- **Fusion Quality**: {output.fusion_quality:.2f}
- **Processing Time**: {output.processing_time:.2f}s

## Recommendations
{await self._generate_processing_recommendations(multimodal_input, output)}
"""
    
    async def _cross_modal_search(self, prompt: str, context: Dict[str, Any]) -> str:
        """Perform cross-modal search and retrieval."""
        query = context.get("query", prompt)
        query_modality = context.get("query_modality", ModalityType.TEXT)
        target_modalities = context.get("target_modalities", [ModalityType.IMAGE, ModalityType.AUDIO])
        search_corpus = context.get("search_corpus", {})
        
        # Create search input
        search_input = {query_modality.value: query}
        
        # Add corpus content for different modalities
        for modality in target_modalities:
            if modality.value in search_corpus:
                search_input[modality.value] = search_corpus[modality.value]
        
        # Process through multimodal engine
        engine_result = await self.multimodal_engine.process(
            search_input,
            {
                "task": MultimodalTask.CROSS_MODAL_RETRIEVAL,
                "query_modality": query_modality,
                "target_modalities": target_modalities
            }
        )
        
        # Rank and format results
        ranked_results = await self._rank_search_results(engine_result, query)
        
        return f"""
# Cross-Modal Search Results

## Query
- **Query**: {query}
- **Query Modality**: {query_modality.value}
- **Target Modalities**: {[m.value for m in target_modalities]}

## Search Results
{await self._format_search_results(ranked_results)}

## Similarity Analysis
{await self._analyze_cross_modal_similarities(engine_result)}

## Search Quality Metrics
- **Total Results**: {len(ranked_results)}
- **Average Similarity**: {np.mean([r.get('similarity', 0) for r in ranked_results]):.3f}
- **Search Confidence**: {engine_result.get('confidence', 0.8):.2f}

## Alternative Queries
{await self._suggest_alternative_queries(query, query_modality)}
"""
    
    async def _generate_multimodal_content(self, prompt: str, context: Dict[str, Any]) -> str:
        """Generate content in one modality based on others."""
        source_content = context.get("source_content", {})
        target_modality = ModalityType(context.get("target_modality", ModalityType.TEXT))
        generation_style = context.get("style", "descriptive")
        
        # Process source content
        if source_content:
            engine_result = await self.multimodal_engine.process(
                source_content,
                {
                    "task": MultimodalTask.CROSS_MODAL_GENERATION,
                    "target_modality": target_modality
                }
            )
        else:
            engine_result = {"embeddings": {}, "fused_representation": []}
        
        # Generate content in target modality
        if target_modality in self.content_generators:
            generated_content = await self.content_generators[target_modality](
                source_content, engine_result, generation_style
            )
        else:
            generated_content = await self._generate_generic_content(
                source_content, target_modality, engine_result
            )
        
        # Evaluate generation quality
        quality_metrics = await self._evaluate_generation_quality(
            source_content, generated_content, target_modality
        )
        
        return f"""
# Multimodal Content Generation

## Source Content Analysis
{await self._analyze_source_content(source_content)}

## Generated Content ({target_modality.value})
{generated_content}

## Generation Quality
{await self._format_quality_metrics(quality_metrics)}

## Style Analysis
- **Generation Style**: {generation_style}
- **Content Length**: {len(str(generated_content))} characters
- **Complexity Score**: {quality_metrics.get('complexity', 0.5):.2f}

## Improvement Suggestions
{await self._suggest_generation_improvements(quality_metrics, target_modality)}
"""
    
    async def _align_semantic_representations(self, prompt: str, context: Dict[str, Any]) -> str:
        """Align semantic representations across modalities."""
        multimodal_content = context.get("content", {})
        alignment_method = context.get("alignment_method", "cosine_similarity")
        
        # Process content through multimodal engine
        engine_result = await self.multimodal_engine.process(
            multimodal_content,
            {
                "task": MultimodalTask.CROSS_MODAL_RETRIEVAL,
                "fusion_strategy": FusionStrategy.ATTENTION_FUSION
            }
        )
        
        # Calculate alignment scores
        alignment_scores = await self._calculate_alignment_scores(
            engine_result.get("embeddings", {}), alignment_method
        )
        
        # Generate alignment analysis
        alignment_analysis = await self._analyze_semantic_alignment(
            alignment_scores, multimodal_content
        )
        
        return f"""
# Semantic Alignment Analysis

## Content Overview
{await self._summarize_multimodal_content(multimodal_content)}

## Alignment Scores
{await self._format_alignment_scores(alignment_scores)}

## Alignment Quality Assessment
{alignment_analysis}

## Cross-Modal Coherence
{await self._assess_cross_modal_coherence(alignment_scores)}

## Optimization Recommendations
{await self._recommend_alignment_optimizations(alignment_scores)}
"""
    
    async def _parse_multimodal_input(self, context: Dict[str, Any]) -> MultimodalInput:
        """Parse and structure multimodal input."""
        modalities = {}
        
        # Extract different modality types from context
        for key, value in context.items():
            if key in ["text", "description", "prompt"]:
                modalities[ModalityType.TEXT] = value
            elif key in ["image", "img", "picture"]:
                modalities[ModalityType.IMAGE] = value
            elif key in ["audio", "sound", "speech"]:
                modalities[ModalityType.AUDIO] = value
            elif key in ["video", "movie", "clip"]:
                modalities[ModalityType.VIDEO] = value
            elif key in ["code", "script", "program"]:
                modalities[ModalityType.CODE] = value
        
        return MultimodalInput(
            id=f"input-{uuid4()}",
            modalities=modalities,
            metadata=context.get("metadata", {}),
            processing_hints=context.get("processing_hints", {}),
            fusion_strategy=FusionStrategy(context.get("fusion_strategy", FusionStrategy.LATE_FUSION))
        )
    
    async def _generate_comprehensive_analysis(self, input_data: MultimodalInput, 
                                             engine_result: Dict[str, Any]) -> str:
        """Generate comprehensive analysis of multimodal content."""
        analysis_parts = []
        
        # Analyze each modality
        for modality, content in input_data.modalities.items():
            modality_analysis = await self._analyze_single_modality(modality, content)
            analysis_parts.append(f"**{modality.value.title()}**: {modality_analysis}")
        
        # Cross-modal analysis
        if len(input_data.modalities) > 1:
            cross_modal_analysis = await self._analyze_cross_modal_relationships(
                input_data.modalities, engine_result
            )
            analysis_parts.append(f"**Cross-Modal Relationships**: {cross_modal_analysis}")
        
        return "\n\n".join(analysis_parts)
    
    def get_capabilities(self) -> List[AgentCapability]:
        """Return agent capabilities."""
        return self.capabilities
    
    async def health_check(self) -> bool:
        """Check agent health."""
        try:
            return await self.multimodal_engine.health_check()
        except:
            return False
    
    # Content generation methods
    async def _generate_text_content(self, source_content: Dict[str, Any], 
                                   engine_result: Dict[str, Any], style: str) -> str:
        """Generate text content from multimodal input."""
        descriptions = []
        
        for modality, content in source_content.items():
            if modality == "image":
                descriptions.append("The image shows a detailed visual scene with various elements and composition.")
            elif modality == "audio":
                descriptions.append("The audio contains distinct sounds and acoustic patterns.")
            elif modality == "video":
                descriptions.append("The video presents a sequence of visual and auditory elements.")
            elif modality == "code":
                descriptions.append(f"The code implements functionality with structured programming logic.")
        
        if style == "technical":
            return f"Technical analysis: {' '.join(descriptions)}"
        elif style == "creative":
            return f"Creative interpretation: {' '.join(descriptions)}"
        else:
            return f"Descriptive analysis: {' '.join(descriptions)}"
    
    async def _generate_image_description(self, source_content: Dict[str, Any], 
                                        engine_result: Dict[str, Any], style: str) -> str:
        """Generate image description from other modalities."""
        if "text" in source_content:
            return f"Visual representation of: {source_content['text'][:100]}..."
        elif "audio" in source_content:
            return "Visual representation of audio waveforms and spectral patterns"
        else:
            return "Generated image description based on multimodal input"
    
    async def _generate_audio_description(self, source_content: Dict[str, Any], 
                                        engine_result: Dict[str, Any], style: str) -> str:
        """Generate audio description from other modalities."""
        return "Audio characteristics: frequency patterns, temporal dynamics, and acoustic features"
    
    async def _generate_code_explanation(self, source_content: Dict[str, Any], 
                                       engine_result: Dict[str, Any], style: str) -> str:
        """Generate code explanation from other modalities."""
        return "Code structure: functions, classes, and algorithmic implementation patterns"
    
    # Helper methods
    async def _calculate_modality_contributions(self, engine_result: Dict[str, Any]) -> Dict[ModalityType, float]:
        """Calculate how much each modality contributed to the result."""
        embeddings = engine_result.get("embeddings", {})
        contributions = {}
        
        total_magnitude = 0
        for modality, embedding in embeddings.items():
            if hasattr(embedding, '__len__'):
                magnitude = sum(abs(x) for x in embedding) if isinstance(embedding, list) else len(embedding)
                contributions[ModalityType(modality)] = magnitude
                total_magnitude += magnitude
        
        # Normalize contributions
        if total_magnitude > 0:
            for modality in contributions:
                contributions[modality] /= total_magnitude
        
        return contributions
    
    async def _format_modality_contributions(self, contributions: Dict[ModalityType, float]) -> str:
        """Format modality contributions for display."""
        formatted = []
        for modality, contribution in contributions.items():
            percentage = contribution * 100
            bar = "â–ˆ" * int(percentage / 5)  # Visual bar
            formatted.append(f"- **{modality.value}**: {percentage:.1f}% {bar}")
        
        return "\n".join(formatted)
    
    # Placeholder implementations for complex methods
    async def _analyze_single_modality(self, modality: ModalityType, content: Any) -> str:
        """Analyze a single modality."""
        return f"Analysis of {modality.value} content with key features and characteristics"
    
    async def _analyze_cross_modal_relationships(self, modalities: Dict[ModalityType, Any], 
                                               engine_result: Dict[str, Any]) -> str:
        """Analyze relationships between modalities."""
        return "Cross-modal relationships show semantic alignment and complementary information"
    
    async def _rank_search_results(self, engine_result: Dict[str, Any], query: str) -> List[Dict[str, Any]]:
        """Rank search results by relevance."""
        return [
            {"content": "Result 1", "similarity": 0.85, "modality": "image"},
            {"content": "Result 2", "similarity": 0.72, "modality": "audio"},
            {"content": "Result 3", "similarity": 0.68, "modality": "text"}
        ]